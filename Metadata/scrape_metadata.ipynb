{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1136466/2158268394.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.autonotebook import tqdm\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go through the bootleg directory and grab the composer and piece names corresponding to each PDF ID number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob(\"imslp_bootleg_dir-v1.1/**/*.pkl\", recursive=True)\n",
    "\n",
    "dir_info = {}\n",
    "\n",
    "for path in file_paths:\n",
    "    path_parts = path.split(\"/\")\n",
    "\n",
    "    composer = path_parts[1]\n",
    "    num = path_parts[-1].replace(\".pkl\", \"\")\n",
    "    piece = \"/\".join(path_parts[2:-1]) # Sometimes the piece name has a slash in it which is why this is necessary - thank you IMSLP\n",
    "    \n",
    "    dir_info[num] = {\"composer\":composer, \"piece_name\":piece, \"path\":path}\n",
    "\n",
    "all_nums = dir_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(num):\n",
    "    composer = dir_info[num][\"composer\"]\n",
    "    piece_name = dir_info[num][\"piece_name\"]\n",
    "\n",
    "    url = f\"https://imslp.org/wiki/{piece_name}({composer})\"\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the info from the IMSLP website for each piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts table and puts entries in web_info (in-place)\n",
    "def extract_table(table, web_info):\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows:\n",
    "        th, td = row.find(\"th\"), row.find(\"td\")\n",
    "        if th and td:\n",
    "            web_info[th.text] = td.text\n",
    "\n",
    "\n",
    "def get_web_info(url):\n",
    "    web_info = {}\n",
    "\n",
    "\n",
    "    # Grab webpage\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "    # Find and extract first table\n",
    "    table = soup.find(\"div\", {\"class\":\"wp_header\"})\n",
    "    if table:\n",
    "        extract_table(table, web_info)\n",
    "\n",
    "\n",
    "    # Find and extract second table\n",
    "    table = soup.find(\"div\", {\"class\":\"wi_body\"})\n",
    "    if table:\n",
    "        extract_table(table, web_info)\n",
    "\n",
    "\n",
    "    # Check if there's a notice\n",
    "    notice = soup.find(\"a\", {\"href\":\"/wiki/File:Ambox_notice.png\"})\n",
    "    if notice:\n",
    "        web_info[\"notice\"] = notice.parent.parent.text\n",
    "\n",
    "\n",
    "    web_info = {k.strip():v.strip() for k, v in web_info.items()}\n",
    "\n",
    "    return web_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "# Parallelize it to go zoom\n",
    "def get_web_infos(urls):\n",
    "    web_infos = {}\n",
    "\n",
    "    def process_url(url):\n",
    "        return url, get_web_info(url)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Using list to keep the order same as urls\n",
    "        results = list(tqdm(executor.map(process_url, urls), total=len(urls)))\n",
    "\n",
    "    for url, web_info in results:\n",
    "        web_infos[url] = web_info\n",
    "\n",
    "    return web_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26998 17239\n"
     ]
    }
   ],
   "source": [
    "urls = {get_url(num) for num in all_nums}\n",
    "# Multiple nums can correspond to the same url (diff versions of the same piece) so only need to scrape once if do it by url\n",
    "print(len(all_nums), len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e63fae793b54ed0a4415006dc9a73e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "web_info = get_web_infos(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the results a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Related Works', 'External Links', 'Instrumentation', 'Manuscript Sources', 'Composition Year', 'Movements/Sections', 'Alternative. Title', 'Dedication', 'InstrDetail', 'I-Catalogue Number', 'Composer Time Period', 'Discography', 'Name Translations', 'Average Duration', 'Language', 'Name Aliases', 'First Performance.', 'Incipit', 'Opus/Catalogue Number', 'notice', 'Key', 'Librettist', 'Composer', 'Copyright Information', 'First Publication', 'Text Incipit', 'Piece Style', 'Genre Categories', 'Work Title', 'Extra Information', 'Authorities', 'Year/Date of Composition', 'First Publication.', 'Extra Locations'}\n"
     ]
    }
   ],
   "source": [
    "all_headers = set()\n",
    "\n",
    "# Clean up slightly (Can't fix this automatically since so inconsistent)\n",
    "fixes = {\n",
    "    \"Average DurationAvg. Duration\":\"Average Duration\",\n",
    "    \"Year/Date of CompositionY/D of Comp.\":\"Year/Date of Composition\",\n",
    "    \"Movements/SectionsMov'ts/Sec's\":\"Movements/Sections\",\n",
    "    \"Opus/Catalogue NumberOp./Cat. No.\":\"Opus/Catalogue Number\",\n",
    "    \"Composer Time PeriodComp. Period\":\"Composer Time Period\",\n",
    "    \"I-Catalogue NumberI-Cat. No.\":\"I-Catalogue Number\",\n",
    "}\n",
    "\n",
    "\n",
    "for url in urls:\n",
    "    # If Key is \"see below\" just get rid of it\n",
    "    if 'Key' in web_info[url] and web_info[url]['Key'] == 'see below':\n",
    "        del web_info[url]['Key']\n",
    "    \n",
    "    for key in list(web_info[url].keys()):\n",
    "        if web_info[url][key] == '':\n",
    "            del web_info[url][key]\n",
    "\n",
    "        elif key in fixes:\n",
    "            new_key = fixes[key]\n",
    "            web_info[url][new_key] = web_info[url][key]\n",
    "            del web_info[url][key]\n",
    "    \n",
    "    all_headers = all_headers.union(set(web_info[url].keys()))\n",
    "\n",
    "print(all_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put it all together and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_pdfids = {url:[] for url in urls}\n",
    "for num in all_nums:\n",
    "    url_to_pdfids[get_url(num)].append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "composers = set(dir_info[num][\"composer\"] for num in all_nums)\n",
    "pieces = set((dir_info[num][\"composer\"], dir_info[num][\"piece_name\"]) for num in all_nums)\n",
    "\n",
    "all_metadata = {composer:{} for composer in composers}\n",
    "for composer, piece_name in pieces:\n",
    "    url = f\"https://imslp.org/wiki/{piece_name}({composer})\"\n",
    "    all_metadata[composer][piece_name] = web_info[url].copy()\n",
    "    all_metadata[composer][piece_name][\"PDF_ids\"] = url_to_pdfids[url]\n",
    "    all_metadata[composer][piece_name][\"url\"] = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Metadata/all_metadata.json\", \"w\") as f:\n",
    "    json.dump(all_metadata, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Metadata/all_metadata.json\", \"r\") as f:\n",
    "    all_metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use it to get 9 way and 100 way metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"9_way_dataset.pkl\", \"rb\") as f:\n",
    "    _, _, _, _, _, _, meta_train, meta_val, meta_test = pickle.load(f)\n",
    "\n",
    "meta_9_nums = [m[0] for m in meta_train + meta_val + meta_test]\n",
    "\n",
    "meta_9_way = {num:all_metadata[dir_info[num]['composer']][dir_info[num]['piece_name']] for num in meta_9_nums}\n",
    "\n",
    "with open(\"Metadata/9_way_metadata.json\", \"w+\") as f:\n",
    "    json.dump(meta_9_way, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"100_way_dataset.pkl\", \"rb\") as f:\n",
    "    _, _, _, _, _, _, meta_train, meta_val, meta_test = pickle.load(f)\n",
    "\n",
    "meta_100_nums = [m[0] for m in meta_train + meta_val + meta_test]\n",
    "\n",
    "meta_100_way = {num:all_metadata[dir_info[num]['composer']][dir_info[num]['piece_name']] for num in meta_100_nums}\n",
    "\n",
    "with open(\"Metadata/100_way_metadata.json\", \"w+\") as f:\n",
    "    json.dump(meta_100_way, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EWLLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
