{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation\n",
    "\n",
    "9-class and 100-class dataset generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from bisect import bisect_left\n",
    "import utils\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directory to save data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"\"  # your username\n",
    "data_path = Path(f\"/home/{username}/ttmp/PBSCR\")  # path to store large files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get bootleg score data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone IMSLP bootleg scores\n",
    "repo_path = data_path / \"piano_bootleg_scores\"\n",
    "!git clone https://github.com/HMC-MIR/piano_bootleg_scores.git {repo_path}\n",
    "piano_bootleg_scores_path = repo_path / \"imslp_bootleg_dir-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random seeding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of composers for 9-class and 100-class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of 9-class and 100-class composers\n",
    "with open(\"9_class_list.txt\", \"r\") as f:\n",
    "    nine_class_composers = f.read().splitlines()\n",
    "with open(\"100_class_list.txt\", \"r\") as f:\n",
    "    hundred_class_composers = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter filler, choose longest PDF for each piece, and create pool of pieces attached to composer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_filler(filler_file, imslp_bootleg_path, filler_threshold=0.5):\n",
    "    composer_dict = {}\n",
    "    with open(filler_file, \"r\") as fin:\n",
    "        lines = fin.readlines()\n",
    "        lines = [line.split(\"\\t\") for line in lines]\n",
    "        for path, page, score in lines:\n",
    "            parts = path.split(\"/\")\n",
    "            composer, piece, id = parts[0], \"/\".join(parts[1:-1]), parts[-1]\n",
    "            composer_dict[composer] = (\n",
    "                {} if composer not in composer_dict else composer_dict[composer]\n",
    "            )\n",
    "            composer_dict[composer][piece] = (\n",
    "                {}\n",
    "                if piece not in composer_dict[composer]\n",
    "                else composer_dict[composer][piece]\n",
    "            )\n",
    "            composer_dict[composer][piece][id] = (\n",
    "                {\"valid_pages\": [], \"count\": 0}\n",
    "                if id not in composer_dict[composer][piece]\n",
    "                else composer_dict[composer][piece][id]\n",
    "            )\n",
    "            if float(score) < filler_threshold:\n",
    "                bscore_page = pd.read_pickle(imslp_bootleg_path / f\"{path}.pkl\")[\n",
    "                    int(page)\n",
    "                ]\n",
    "                composer_dict[composer][piece][id][\"valid_pages\"].append(int(page))\n",
    "                composer_dict[composer][piece][id][\"count\"] += len(bscore_page)\n",
    "    return composer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HELPER\n",
    "def ints_to_binary_matrix(score_seq):  # converts integer sequence to n x 62 matrix\n",
    "    matrix = []\n",
    "    for event in score_seq:\n",
    "        binary_rep = list(np.binary_repr(event, 62))\n",
    "        matrix.append(binary_rep)\n",
    "    np_mat = np.array(matrix, dtype=np.uint8)\n",
    "    # np_mat = np.flip(np_mat, axis=0)  # flip to have least significant bit at the front\n",
    "    return np_mat\n",
    "\n",
    "\n",
    "# Filter out filler and choose longest score PDF for each unique piece\n",
    "composer_dict = process_filler(\n",
    "    \"filler.tsv\", piano_bootleg_scores_path, filler_threshold=0.5\n",
    ")\n",
    "valid_pdfs = {}\n",
    "for composer in composer_dict:\n",
    "    for piece in composer_dict[composer]:\n",
    "        max_count = 0\n",
    "        for id in composer_dict[composer][piece]:\n",
    "            if composer_dict[composer][piece][id][\"count\"] > max_count:\n",
    "                max_count = composer_dict[composer][piece][id][\"count\"]\n",
    "                valid_pdfs[composer] = (\n",
    "                    {} if composer not in valid_pdfs else valid_pdfs[composer]\n",
    "                )\n",
    "                valid_pdfs[composer][piece] = {\n",
    "                    \"id\": id,\n",
    "                    \"valid_pages\": composer_dict[composer][piece][id][\"valid_pages\"],\n",
    "                    \"count\": max_count,\n",
    "                }\n",
    "\n",
    "# Create pool of bootleg score binary matrices\n",
    "# List of tuples containing (binary_score, composer)\n",
    "pieces = []\n",
    "# for composer in list(set(nine_class_composers) | set(hundred_class_composers)): # only take required composers\n",
    "for composer in tqdm(list(set(nine_class_composers) | set(hundred_class_composers))):\n",
    "    for piece in valid_pdfs[composer]:\n",
    "        pkl = (\n",
    "            piano_bootleg_scores_path\n",
    "            / f\"{composer}/{piece}/{valid_pdfs[composer][piece]['id']}.pkl\"\n",
    "        )\n",
    "        page_scores = pd.read_pickle(pkl)\n",
    "\n",
    "        valid_pages = valid_pdfs[composer][piece][\"valid_pages\"]\n",
    "        bscores = []\n",
    "        for page in valid_pages:\n",
    "            page_score = page_scores[page]\n",
    "            bscores.append(ints_to_binary_matrix(page_score))\n",
    "        bscores = [\n",
    "            page for page in bscores if len(page.shape) == 2 and page.shape[1] == 62\n",
    "        ]\n",
    "        # pieces.append((bscores, composer, pkl))\n",
    "        piece = np.concatenate(bscores, axis=0)\n",
    "        pieces.append((piece, composer, pkl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"100_class_list.txt\") as f:\n",
    "    hundred_class_composers = f.read().splitlines()\n",
    "    piece_count = 0\n",
    "    for composer in hundred_class_composers:\n",
    "        piece_count += len(composer_dict[composer])\n",
    "\n",
    "print(\"Piece count:\", piece_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"100_class_list.txt\") as f:\n",
    "    hundred_class_composers = f.read().splitlines()\n",
    "    page_count = 0\n",
    "    for composer in hundred_class_composers:\n",
    "        for valid_pdf in valid_pdfs[composer]:\n",
    "            d = pd.read_pickle(\n",
    "                piano_bootleg_scores_path\n",
    "                / f\"{composer}/{valid_pdf}/{valid_pdfs[composer][valid_pdf]['id']}.pkl\"\n",
    "            )\n",
    "            page_count += len(d)\n",
    "print(\"Page count:\", page_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"100_class_list.txt\") as f:\n",
    "    hundred_class_composers = f.read().splitlines()\n",
    "    nonfiller_page_count = 0\n",
    "    for composer in hundred_class_composers:\n",
    "        for valid_pdf in valid_pdfs[composer]:\n",
    "            nonfiller_page_count += len(valid_pdfs[composer][valid_pdf][\"valid_pages\"])\n",
    "\n",
    "print(\"Page count:\", nonfiller_page_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"100_class_list.txt\") as f:\n",
    "    hundred_class_composers = f.read().splitlines()\n",
    "    bscore_count = 0\n",
    "    for composer in hundred_class_composers:\n",
    "        for valid_pdf in valid_pdfs[composer]:\n",
    "            bscore_count += valid_pdfs[composer][valid_pdf][\"count\"]\n",
    "\n",
    "print(\"Bscore count:\", bscore_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new IMSLP dataset with filler pages replaced with []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(str(piano_bootleg_scores_path) + \".1\", exist_ok=True)\n",
    "\n",
    "for composer, info in composer_dict.items():\n",
    "    os.makedirs(str(piano_bootleg_scores_path) + \".1/\" + composer, exist_ok=True)\n",
    "    for piece_name, piece_info in info.items():\n",
    "        os.makedirs(\n",
    "            str(piano_bootleg_scores_path) + \".1/\" + composer + \"/\" + piece_name,\n",
    "            exist_ok=True,\n",
    "        )\n",
    "        for id_, valid_info in piece_info.items():\n",
    "            fname = (\n",
    "                str(piano_bootleg_scores_path)\n",
    "                + \".1/\"\n",
    "                + composer\n",
    "                + \"/\"\n",
    "                + piece_name\n",
    "                + \"/\"\n",
    "                + id_\n",
    "                + \".pkl\"\n",
    "            )\n",
    "            old_fname = (\n",
    "                str(piano_bootleg_scores_path)\n",
    "                + \"/\"\n",
    "                + composer\n",
    "                + \"/\"\n",
    "                + piece_name\n",
    "                + \"/\"\n",
    "                + id_\n",
    "                + \".pkl\"\n",
    "            )\n",
    "            valid_pages = valid_info[\"valid_pages\"]\n",
    "\n",
    "            with open(old_fname, \"rb\") as f:\n",
    "                pages = pickle.load(f)\n",
    "\n",
    "            new_pages = [\n",
    "                page if i in valid_pages else [] for i, page in enumerate(pages)\n",
    "            ]\n",
    "\n",
    "            with open(fname, \"wb\") as f:\n",
    "                pickle.dump(new_pages, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for sampling fragments for dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffles three lists the same class\n",
    "def co_shuffle(list1, list2, list3):\n",
    "    temp = list(zip(list1, list2, list3))\n",
    "    np.random.shuffle(temp)\n",
    "    res1, res2, res3 = zip(*temp)\n",
    "    res1, res2, res3 = list(res1), list(res2), list(res3)\n",
    "    return res1, res2, res3\n",
    "\n",
    "\n",
    "def fragment_data(pieces, composer_list, samples=60_000, fragment_len=64):\n",
    "    \"\"\"Takes list of (binary_matrix, composer) and creates fragments of fragment_len based on it.\n",
    "    pieces: The list of (binary_matrix, composer) to sample from\n",
    "    samles: The number of samples to gather\n",
    "    fragment_len: The length of each fragment\n",
    "    \"\"\"\n",
    "\n",
    "    # Organize pieces by composer\n",
    "    composer_pieces = {}\n",
    "    for (piece, path), composer in pieces:\n",
    "        if len(piece) > 64:\n",
    "            if not composer in composer_pieces:\n",
    "                composer_pieces[composer] = []\n",
    "            composer_pieces[composer].append((piece, path))\n",
    "\n",
    "    x_fragments = []\n",
    "    y_fragments = []\n",
    "    metadata = []\n",
    "\n",
    "    fragments_per_composer = round(samples / len(composer_list))\n",
    "    for composer, piece_list in composer_pieces.items():\n",
    "        if not composer in composer_list:\n",
    "            continue\n",
    "\n",
    "        for _ in range(fragments_per_composer):\n",
    "            # Get random piece by that composer\n",
    "            piece, path = random.choice(piece_list)\n",
    "\n",
    "            # Get random fragment from piece\n",
    "            start = np.random.randint(len(piece) - fragment_len)\n",
    "            fragment = piece[start : start + fragment_len].copy()\n",
    "\n",
    "            # Get page num to start fragment from\n",
    "            d = pd.read_pickle(path)\n",
    "            psum = []\n",
    "            for page in d:\n",
    "                psum.append(len(page))\n",
    "            psum = np.cumsum(psum)\n",
    "            psum = [x - 1 for x in psum]\n",
    "            page = bisect_left(psum, start)\n",
    "            page_offset = start - psum[page - 1] if page > 0 else start\n",
    "\n",
    "            x_fragments.append(fragment)\n",
    "            y_fragments.append(composer)\n",
    "            metadata.append((path, page, page_offset))\n",
    "\n",
    "    return np.stack(x_fragments), np.stack(y_fragments), metadata\n",
    "\n",
    "\n",
    "def create_fragment_dataset(\n",
    "    pieces,\n",
    "    composer_list,\n",
    "    valid_split=0.15,\n",
    "    test_split=0.15,\n",
    "    samples=60_000,\n",
    "    fragment_len=64,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a train / Test split dataset of fragments.\n",
    "    pieces: The list of (binary_matrix, composer) to sample from\n",
    "    split: The proportion of data to use to test and valid (each get this proportion and the rest is for train - split=.1 -> train=.8, test=.1, valid=.1)\n",
    "    samples: The number of samples to gather (train + test)\n",
    "    fragment_len: The length of each fragment\n",
    "    \"\"\"\n",
    "\n",
    "    composer_pieces = {\n",
    "        composer1: [\n",
    "            (piece, path)\n",
    "            for piece, composer2, path in pieces\n",
    "            if composer2 == composer1 and len(piece) > fragment_len\n",
    "        ]\n",
    "        for composer1 in composer_list\n",
    "    }\n",
    "    # Go through each composer and separate pieces into train, valid, test\n",
    "    train_pieces = []\n",
    "    valid_pieces = []\n",
    "    test_pieces = []\n",
    "    for composer in composer_list:\n",
    "        # for composer, piece_list in composer_pieces.items():\n",
    "        piece_list = composer_pieces[composer]\n",
    "        np.random.shuffle(piece_list)\n",
    "\n",
    "        # Make sure each piece is matched to the composer\n",
    "        piece_list = list(zip(piece_list, [composer] * len(piece_list)))\n",
    "\n",
    "        # Calculate starting places of each section - order is (test, valid, train)\n",
    "        train_start = round((valid_split + test_split) * len(piece_list))\n",
    "        valid_start = round(test_split * len(piece_list))\n",
    "\n",
    "        # Add composer info and add each part to its respective set\n",
    "        train_pieces += piece_list[train_start:]\n",
    "        valid_pieces += piece_list[valid_start:train_start]\n",
    "        test_pieces += piece_list[:valid_start]\n",
    "\n",
    "    # Fragment the pieces\n",
    "    x_train_fragments, y_train_fragments, meta_train = fragment_data(\n",
    "        train_pieces,\n",
    "        composer_list,\n",
    "        samples=round((1 - (valid_split + test_split)) * samples),\n",
    "        fragment_len=fragment_len,\n",
    "    )\n",
    "    x_valid_fragments, y_valid_fragments, meta_valid = fragment_data(\n",
    "        valid_pieces,\n",
    "        composer_list,\n",
    "        samples=round(valid_split * samples),\n",
    "        fragment_len=fragment_len,\n",
    "    )\n",
    "    x_test_fragments, y_test_fragments, meta_test = fragment_data(\n",
    "        test_pieces,\n",
    "        composer_list,\n",
    "        samples=round(test_split * samples),\n",
    "        fragment_len=fragment_len,\n",
    "    )\n",
    "\n",
    "    # Reshuffle pieces\n",
    "    x_train_fragments, y_train_fragments, meta_train = co_shuffle(\n",
    "        x_train_fragments, y_train_fragments, meta_train\n",
    "    )\n",
    "    x_valid_fragments, y_valid_fragments, meta_valid = co_shuffle(\n",
    "        x_valid_fragments, y_valid_fragments, meta_valid\n",
    "    )\n",
    "    x_test_fragments, y_test_fragments, meta_test = co_shuffle(\n",
    "        x_test_fragments, y_test_fragments, meta_test\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        np.stack(x_train_fragments),\n",
    "        np.stack(y_train_fragments),\n",
    "        meta_train,\n",
    "        np.stack(x_valid_fragments),\n",
    "        np.stack(y_valid_fragments),\n",
    "        meta_valid,\n",
    "        np.stack(x_test_fragments),\n",
    "        np.stack(y_test_fragments),\n",
    "        meta_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and save 9-class dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    x_train_fragments,\n",
    "    y_train_fragments,\n",
    "    meta_train,\n",
    "    x_valid_fragments,\n",
    "    y_valid_fragments,\n",
    "    meta_valid,\n",
    "    x_test_fragments,\n",
    "    y_test_fragments,\n",
    "    meta_test,\n",
    ") = create_fragment_dataset(pieces, nine_class_composers, samples=40_000)\n",
    "\n",
    "with open(data_path / \"9_class_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(\n",
    "        (\n",
    "            x_train_fragments,\n",
    "            y_train_fragments,\n",
    "            x_valid_fragments,\n",
    "            y_valid_fragments,\n",
    "            x_test_fragments,\n",
    "            y_test_fragments,\n",
    "            meta_train,\n",
    "            meta_valid,\n",
    "            meta_test,\n",
    "        ),\n",
    "        f,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and save 100-class dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    x_train_fragments,\n",
    "    y_train_fragments,\n",
    "    meta_train,\n",
    "    x_valid_fragments,\n",
    "    y_valid_fragments,\n",
    "    meta_valid,\n",
    "    x_test_fragments,\n",
    "    y_test_fragments,\n",
    "    meta_test,\n",
    ") = create_fragment_dataset(pieces, hundred_class_composers, samples=100_000)\n",
    "\n",
    "with open(data_path / \"100_class_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(\n",
    "        (\n",
    "            x_train_fragments,\n",
    "            y_train_fragments,\n",
    "            x_valid_fragments,\n",
    "            y_valid_fragments,\n",
    "            x_test_fragments,\n",
    "            y_test_fragments,\n",
    "            meta_train,\n",
    "            meta_valid,\n",
    "            meta_test,\n",
    "        ),\n",
    "        f,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
