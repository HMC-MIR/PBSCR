{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation\n",
    "\n",
    "9-way and 100-way dataset generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from bisect import bisect_left\n",
    "import utils\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directory to save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/home/ajain/ttmp/PBSCSR_data/\")  # path to store large files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get bootleg score data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path '/home/ajain/ttmp/PBSCSR_data/piano_bootleg_scores' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# Clone IMSLP bootleg scores\n",
    "repo_path = data_path/\"piano_bootleg_scores\"\n",
    "!git clone https://github.com/HMC-MIR/piano_bootleg_scores.git {repo_path}\n",
    "piano_bootleg_scores_path = repo_path/\"imslp_bootleg_dir-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of composers for 9-way and 100-way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of 9-way and 100-way composers\n",
    "with open('../9_way_list.txt', \"r\") as f:\n",
    "    nine_way_composers = f.read().splitlines()\n",
    "with open('../100_way_list.txt', \"r\") as f:\n",
    "    hundred_way_composers = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter filler, choose longest PDF for each piece, and create pool of pieces attached to composer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_filler(filler_file, imslp_bootleg_path, filler_threshold=0.5):\n",
    "    composer_dict = {}\n",
    "    with open(filler_file, \"r\") as fin:\n",
    "        lines = fin.readlines()\n",
    "        lines = [line.split(\"\\t\") for line in lines]\n",
    "        for path, page, score in lines:\n",
    "            parts = path.split(\"/\")\n",
    "            composer, piece, id = parts[0], \"/\".join(parts[1:-1]), parts[-1]\n",
    "            composer_dict[composer] = {} if composer not in composer_dict else composer_dict[composer]\n",
    "            composer_dict[composer][piece] = {} if piece not in composer_dict[composer] else composer_dict[composer][piece]\n",
    "            composer_dict[composer][piece][id] = {\"valid_pages\":[], \"count\":0} if id not in composer_dict[composer][piece] else composer_dict[composer][piece][id]\n",
    "            if float(score) < filler_threshold:\n",
    "                bscore_page = pd.read_pickle(imslp_bootleg_path/f\"{path}.pkl\")[int(page)]\n",
    "                composer_dict[composer][piece][id][\"valid_pages\"].append(int(page))\n",
    "                composer_dict[composer][piece][id][\"count\"] += len(bscore_page)\n",
    "    return composer_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f64fd0b0e164d309a92e0727b55f2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## HELPER\n",
    "def ints_to_binary_matrix(score_seq):  # converts integer sequence to n x 62 matrix\n",
    "    matrix = []\n",
    "    for event in score_seq:\n",
    "        binary_rep = list(np.binary_repr(event, 62))\n",
    "        matrix.append(binary_rep)\n",
    "    np_mat = np.array(matrix, dtype=np.uint8)\n",
    "    #np_mat = np.flip(np_mat, axis=0)  # flip to have least significant bit at the front\n",
    "    return np_mat\n",
    "\n",
    "    \n",
    "# Filter out filler and choose longest score PDF for each unique piece\n",
    "composer_dict = process_filler(\"../filler.tsv\", piano_bootleg_scores_path, filler_threshold=0.5)\n",
    "valid_pdfs = {}\n",
    "for composer in composer_dict:\n",
    "    for piece in composer_dict[composer]:\n",
    "        max_count = 0\n",
    "        for id in composer_dict[composer][piece]:\n",
    "            if composer_dict[composer][piece][id][\"count\"] > max_count:\n",
    "                max_count = composer_dict[composer][piece][id][\"count\"]\n",
    "                valid_pdfs[composer] = {} if composer not in valid_pdfs else valid_pdfs[composer]\n",
    "                valid_pdfs[composer][piece] = {\"id\": id,\n",
    "                                               \"valid_pages\": composer_dict[composer][piece][id][\"valid_pages\"],\n",
    "                                               \"count\": max_count}\n",
    "\n",
    "# Create pool of bootleg score binary matrices\n",
    "# List of tuples containing (binary_score, composer)\n",
    "pieces = []\n",
    "# for composer in list(set(nine_way_composers) | set(hundred_way_composers)): # only take required composers\n",
    "for composer in tqdm(list(set(nine_way_composers) | set(hundred_way_composers))):\n",
    "    for piece in valid_pdfs[composer]:\n",
    "        pkl = piano_bootleg_scores_path/f\"{composer}/{piece}/{valid_pdfs[composer][piece]['id']}.pkl\"\n",
    "        page_scores  = pd.read_pickle(pkl)\n",
    "\n",
    "        valid_pages = valid_pdfs[composer][piece][\"valid_pages\"]\n",
    "        bscores = []\n",
    "        for page in valid_pages:\n",
    "            page_score = page_scores[page]\n",
    "            bscores.append(ints_to_binary_matrix(page_score))\n",
    "        bscores = [page for page in bscores if len(page.shape) == 2 and page.shape[1] == 62]\n",
    "        \n",
    "        piece = np.concatenate(bscores, axis=0)\n",
    "        pieces.append((piece, composer, pkl))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piece count: 4997\n"
     ]
    }
   ],
   "source": [
    "with open(\"../100_way_list.txt\") as f:\n",
    "    hundred_way_composers = f.read().splitlines()\n",
    "    piece_count = 0\n",
    "    for composer in hundred_way_composers:\n",
    "        piece_count += len(composer_dict[composer])\n",
    "\n",
    "print(\"Piece count:\", piece_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page count: 70440\n"
     ]
    }
   ],
   "source": [
    "with open(\"../100_way_list.txt\") as f:\n",
    "    hundred_way_composers = f.read().splitlines()\n",
    "    page_count = 0\n",
    "    for composer in hundred_way_composers:\n",
    "        for valid_pdf in valid_pdfs[composer]:\n",
    "            d = pd.read_pickle(piano_bootleg_scores_path/f\"{composer}/{valid_pdf}/{valid_pdfs[composer][valid_pdf]['id']}.pkl\")\n",
    "            page_count += len(d)\n",
    "print(\"Page count:\", page_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page count: 64129\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"../100_way_list.txt\") as f:\n",
    "    hundred_way_composers = f.read().splitlines()\n",
    "    nonfiller_page_count = 0\n",
    "    for composer in hundred_way_composers:\n",
    "        for valid_pdf in valid_pdfs[composer]:\n",
    "            nonfiller_page_count += len(valid_pdfs[composer][valid_pdf][\"valid_pages\"])\n",
    "\n",
    "print(\"Page count:\", nonfiller_page_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bscore count: 12108749\n"
     ]
    }
   ],
   "source": [
    "with open(\"../100_way_list.txt\") as f:\n",
    "    hundred_way_composers = f.read().splitlines()\n",
    "    bscore_count = 0\n",
    "    for composer in hundred_way_composers:\n",
    "        for valid_pdf in valid_pdfs[composer]:\n",
    "            bscore_count += valid_pdfs[composer][valid_pdf][\"count\"]\n",
    "\n",
    "print(\"Bscore count:\", bscore_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new IMSLP dataset with filler pages replaced with []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(str(piano_bootleg_scores_path)+\".1\", exist_ok=True)\n",
    "\n",
    "for composer, info in composer_dict.items():\n",
    "    os.makedirs(str(piano_bootleg_scores_path)+\".1/\"+composer, exist_ok=True)\n",
    "    for piece_name, piece_info in info.items():\n",
    "        os.makedirs(str(piano_bootleg_scores_path)+\".1/\"+composer+\"/\"+piece_name, exist_ok=True)\n",
    "        for id_, valid_info in piece_info.items():\n",
    "            fname = str(piano_bootleg_scores_path)+\".1/\"+composer+\"/\"+piece_name+\"/\"+id_+\".pkl\"\n",
    "            old_fname = str(piano_bootleg_scores_path)+\"/\"+composer+\"/\"+piece_name+\"/\"+id_+\".pkl\"\n",
    "            valid_pages = valid_info[\"valid_pages\"]\n",
    "\n",
    "            with open(old_fname, \"rb\") as f:\n",
    "                pages = pickle.load(f)\n",
    "\n",
    "            new_pages = [page if i in valid_pages else [] for i, page in enumerate(pages)]\n",
    "\n",
    "            with open(fname, \"wb\") as f:\n",
    "                pickle.dump(new_pages, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for sampling fragments for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Shuffles two lists the same way\n",
    "def co_shuffle(list1, list2, list3):\n",
    "    temp = list(zip(list1, list2, list3))\n",
    "    np.random.shuffle(temp)\n",
    "    res1, res2, res3 = zip(*temp)\n",
    "    res1, res2, res3 = list(res1), list(res2), list(res3)\n",
    "    return res1, res2, res3\n",
    "\n",
    "def fragment_data(pieces, composer_list, samples=60_000, fragment_len=64):\n",
    "    \"\"\"Takes list of (binary_matrix, composer) and creates fragments of fragment_len based on it.\n",
    "    pieces: The list of (binary_matrix, composer) to sample from\n",
    "    samles: The number of samples to gather\n",
    "    fragment_len: The length of each fragment\n",
    "    \"\"\"\n",
    "\n",
    "    # Organize pieces by composer\n",
    "    composer_pieces = {}\n",
    "    for (piece, path), composer in pieces:\n",
    "        if len(piece) > 64:\n",
    "            if not composer in composer_pieces:\n",
    "                composer_pieces[composer] = []\n",
    "            composer_pieces[composer].append((piece, path))\n",
    "\n",
    "    x_fragments = []\n",
    "    y_fragments = []\n",
    "    metadata = []\n",
    "    \n",
    "    fragments_per_composer = round(samples / len(composer_list))\n",
    "    for composer, piece_list  in composer_pieces.items():\n",
    "        if not composer in composer_list:\n",
    "            continue\n",
    "\n",
    "        for i in range(fragments_per_composer):\n",
    "            # Get random piece by that composer\n",
    "            piece, path = random.choice(piece_list)\n",
    "\n",
    "            # Get random fragment from piece\n",
    "            start = np.random.randint(len(piece)-fragment_len)\n",
    "            fragment = piece[start:start+fragment_len].copy()\n",
    "            \n",
    "            # Get page num to start fragment from\n",
    "            d = pd.read_pickle(path)\n",
    "            psum = []\n",
    "            for page in d:\n",
    "                psum.append(len(page))\n",
    "            psum = np.cumsum(psum)\n",
    "            psum = [x-1 for x in psum]\n",
    "            page = bisect_left(psum, start)\n",
    "            page_offset = start - psum[page-1] if page > 0 else start\n",
    "            x_fragments.append(fragment)\n",
    "            y_fragments.append(composer)\n",
    "            metadata.append((Path(path).stem, page, page_offset))\n",
    "                \n",
    "    return x_fragments, y_fragments, metadata\n",
    "        \n",
    "\n",
    "def create_fragment_dataset(pieces, composer_list, valid_split = 0.15, test_split = 0.15, samples=60_000, fragment_len=64):\n",
    "    \"\"\"\n",
    "    Creates a train / Test split dataset of fragments.\n",
    "    pieces: The list of (binary_matrix, composer) to sample from\n",
    "    split: The proportion of data to use to test and valid (each get this proportion and the rest is for train - split=.1 -> train=.8, test=.1, valid=.1)\n",
    "    samples: The number of samples to gather (train + test)\n",
    "    fragment_len: The length of each fragment\n",
    "    \"\"\"\n",
    "    \n",
    "    composer_pieces = {composer1:[(piece, path) for piece, composer2, path in pieces if composer2 == composer1 and len(piece) > fragment_len] for composer1 in composer_list}\n",
    "    # Go through each composer and separate pieces into train, valid, test\n",
    "    train_pieces = []\n",
    "    valid_pieces = []\n",
    "    test_pieces = []\n",
    "    for composer in composer_list:\n",
    "    # for composer, piece_list in composer_pieces.items():\n",
    "        piece_list = composer_pieces[composer]\n",
    "        np.random.shuffle(piece_list)\n",
    "        \n",
    "        # Make sure each piece is matched to the composer\n",
    "        piece_list = list(zip(piece_list, [composer]*len(piece_list)))\n",
    "        \n",
    "        # Calculate starting places of each section - order is (test, valid, train)\n",
    "        train_start = round((valid_split+test_split)*len(piece_list))\n",
    "        valid_start = round(test_split*len(piece_list))\n",
    "        \n",
    "        # Add composer info and add each part to its respective set\n",
    "        train_pieces += piece_list[train_start:]\n",
    "        valid_pieces += piece_list[valid_start:train_start]\n",
    "        test_pieces += piece_list[:valid_start]\n",
    "\n",
    "    # with open(\"composer_pieces.pkl\", \"wb\") as f:\n",
    "    #     pickle.dump(train_pieces, f)\n",
    "     # Fragment the pieces\n",
    "    x_train_fragments, y_train_fragments, meta_train = fragment_data(train_pieces, composer_list, samples=round((1-(valid_split+test_split))*samples), fragment_len=fragment_len)\n",
    "    x_valid_fragments, y_valid_fragments, meta_valid = fragment_data(valid_pieces, composer_list, samples=round(valid_split*samples), fragment_len=fragment_len)\n",
    "    x_test_fragments, y_test_fragments, meta_test = fragment_data(test_pieces, composer_list, samples=round(test_split*samples), fragment_len=fragment_len)\n",
    "    \n",
    "    # Reshuffle pieces\n",
    "    x_train_fragments, y_train_fragments, meta_train = co_shuffle(x_train_fragments, y_train_fragments, meta_train)\n",
    "    x_valid_fragments, y_valid_fragments, meta_valid = co_shuffle(x_valid_fragments, y_valid_fragments, meta_valid)\n",
    "    x_test_fragments, y_test_fragments, meta_test = co_shuffle(x_test_fragments, y_test_fragments, meta_test) \n",
    "    \n",
    "    return x_train_fragments, y_train_fragments, meta_train, x_valid_fragments, y_valid_fragments, meta_valid, x_test_fragments, y_test_fragments, meta_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and save 9-way dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_fragments, y_train_fragments, meta_train, \\\n",
    "    x_valid_fragments, y_valid_fragments, meta_valid, \\\n",
    "    x_test_fragments, y_test_fragments, meta_test = create_fragment_dataset(pieces, nine_way_composers, samples=40_000)\n",
    "\n",
    "with open(data_path/\"9_way_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump((x_train_fragments, y_train_fragments,\n",
    "                 x_valid_fragments, y_valid_fragments,\n",
    "                 x_test_fragments, y_test_fragments,\n",
    "                 meta_train, meta_valid, meta_test), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and save 100-way dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_fragments, y_train_fragments, meta_train, \\\n",
    "    x_valid_fragments, y_valid_fragments, meta_valid, \\\n",
    "    x_test_fragments, y_test_fragments, meta_test = create_fragment_dataset(pieces, hundred_way_composers, samples=100_000)\n",
    "\n",
    "with open(data_path/\"100_way_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump((x_train_fragments, y_train_fragments, x_valid_fragments, y_valid_fragments, x_test_fragments, y_test_fragments), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EWLLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
