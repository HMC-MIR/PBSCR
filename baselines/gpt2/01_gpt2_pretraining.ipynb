{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Training Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Currently only tested for GPT2!* "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "# import evaluate\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.normalizers import (Sequence, Lowercase, NFD, StripAccents)\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.decoders import BPEDecoder\n",
    "from transformers import AutoConfig, \\\n",
    "    DataCollatorWithPadding, AutoModelForSequenceClassification, \\\n",
    "    Trainer, TrainingArguments, AutoTokenizer, GPT2Config\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/home/ajain/ttmp/PBSCSR_data/\")\n",
    "'''Path to large data folder'''\n",
    "\n",
    "seed = 42\n",
    "'''Random seed: int'''\n",
    "\n",
    "gpt2_dir = data_path/\"gpt2\"\n",
    "gpt2_dir.mkdir(exist_ok=True)\n",
    "\n",
    "train_file = gpt2_dir/\"LM_pretraining_data/train.txt\"\n",
    "train_file.parent.mkdir(exist_ok=True)\n",
    "valid_file = gpt2_dir/\"LM_pretraining_data/valid.txt\"\n",
    "\n",
    "pretrained_output = gpt2_dir/\"pretrained_model\"\n",
    "pretrained_output.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "cache_dir = data_path/\".cache\"\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "'''Path to save cache'''\n",
    "\n",
    "labeled_data_path = \"../../9_way_dataset.zip\"  # TODO: FILL IN AFTER UNZIPPING\n",
    "'''Path to labeled data'''\n",
    "classifier_output_dir = gpt2_dir/\"classifier\"\n",
    "classifier_output_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_filler(filler_file, imslp_bootleg_path, filler_threshold=0.5):\n",
    "    composer_dict = {}\n",
    "    with open(filler_file, \"r\") as fin:\n",
    "        lines = fin.readlines()\n",
    "        lines = [line.split(\"\\t\") for line in lines]\n",
    "        for path, page, score in lines:\n",
    "            parts = path.split(\"/\")\n",
    "            composer, piece, id = parts[0], \"/\".join(parts[1:-1]), parts[-1]\n",
    "            composer_dict[composer] = {} if composer not in composer_dict else composer_dict[composer]\n",
    "            composer_dict[composer][piece] = {} if piece not in composer_dict[composer] else composer_dict[composer][piece]\n",
    "            composer_dict[composer][piece][id] = {\"valid_pages\":[], \"count\":0, \"bscore\": []} if id not in composer_dict[composer][piece] else composer_dict[composer][piece][id]\n",
    "            if float(score) < filler_threshold:\n",
    "                bscore_page = pd.read_pickle(imslp_bootleg_path/f\"{path}.pkl\")[int(page)]\n",
    "                composer_dict[composer][piece][id][\"valid_pages\"].append(int(page))\n",
    "                composer_dict[composer][piece][id][\"count\"] += len(bscore_page)\n",
    "                composer_dict[composer][piece][id][\"bscore\"].append(bscore_page)\n",
    "    return composer_dict\n",
    "\n",
    "\n",
    "def ints_to_binary_matrix(score_seq):  # converts integer sequence to n x 62 matrix\n",
    "    matrix = []\n",
    "    for event in score_seq:\n",
    "        binary_rep = list(np.binary_repr(event, 62))\n",
    "        matrix.append(binary_rep)\n",
    "    np_mat = np.array(matrix, dtype=np.uint8)\n",
    "    #np_mat = np.flip(np_mat, axis=0)  # flip to have least significant bit at the front\n",
    "    return np_mat\n",
    "\n",
    "\n",
    "def create_dataset(pieces, valid_split=.15, test_split=.15):\n",
    "    \"\"\"\n",
    "    Creates a train / valid / test split dataset of pieces.\n",
    "    pieces: The list of binary_matrices to sample from\n",
    "    valid_split: The proportion of data to use for valid\n",
    "    test_split: The proportion of data to use for valid\n",
    "    \n",
    "    returns:\n",
    "    x & y lists for train, valid, and test sets\n",
    "    \"\"\"\n",
    "    \n",
    "    # For repeatability\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # shuffle pieces\n",
    "    piece_list = [piece for piece in pieces]\n",
    "    np.random.shuffle(piece_list)\n",
    "    \n",
    "    # Calculate starting places of each section - order is (test, valid, train)\n",
    "    train_start = round((valid_split+test_split)*len(piece_list))\n",
    "    valid_start = round(test_split*len(piece_list))\n",
    "    \n",
    "    # Go through and separate pieces into train, valid, test\n",
    "    train_pieces = piece_list[train_start:]\n",
    "    valid_pieces = piece_list[valid_start:train_start]\n",
    "    test_pieces = piece_list[:valid_start]\n",
    "    \n",
    "    return train_pieces, valid_pieces, test_pieces\n",
    "\n",
    "def merge_staff_overlaps(bscores):\n",
    "    \"\"\"\n",
    "    Takes in either one binary score or a batch of them and merges the left and right hands\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lower middle c is index 23\n",
    "    # Upper middle c is index 33\n",
    "    lower = 23\n",
    "    upper = 33\n",
    "    middle = (lower + upper) // 2\n",
    "    \n",
    "    # Total notes is 52\n",
    "    total = 52\n",
    "    \n",
    "    # Pad out upper hand and lower hand and combine them\n",
    "    padded_lower = np.concatenate([bscores[..., :middle], np.zeros((*bscores.shape[:-1], total-middle))], axis=-1)\n",
    "    padded_upper = np.concatenate([np.zeros((*bscores.shape[:-1], middle-bscores.shape[-1]+total)), bscores[..., middle:]], axis=-1)\n",
    "    # Logical or\n",
    "    merged = padded_lower + padded_upper - padded_lower * padded_upper\n",
    "    return merged\n",
    "\n",
    "\n",
    "# Dense Encoder\n",
    "\n",
    "# Continuous line of 256 unicode characters\n",
    "start = 10060# 931\n",
    "dense_characters = [chr(i).encode(\"utf-8\").decode(\"utf-8\") for i in range(start, start+512)]\n",
    "\n",
    "\n",
    "# This code divides the fragment into blocks (and discards any remaining info at the very edges)\n",
    "# Then it uses einsum with a filter of powers of 2 to convert from binary to an integer.  Then converts integers into\n",
    "# unicode characters\n",
    "\n",
    "def dense_encoder(fragment, block_size=[1, 8]):\n",
    "    fragment = merge_staff_overlaps(fragment)\n",
    "    # Rewrote this to be much faster but looks complicated\n",
    "    # This filter has powers of 2 which is how the binary is turned to ints\n",
    "    filter_ = np.power(2, np.arange(np.prod(block_size))).reshape(block_size)\n",
    "    \n",
    "    # The fragment is split into blocks here\n",
    "    xblocks = np.stack(np.split(fragment[:, :(fragment.shape[1]//block_size[1])*block_size[1]], fragment.shape[1]//block_size[1], axis=1))\n",
    "    xyblocks = np.stack(np.split(xblocks[:, :(xblocks.shape[1]//block_size[0])*block_size[0]], xblocks.shape[1]//block_size[0], axis=1))\n",
    "    \n",
    "    # The blocks are multiplied so they are ints\n",
    "    numbers = np.einsum(\"ijkl,kl->ij\", xyblocks, filter_)\n",
    "    \n",
    "    # The ints are turned into corresponding characters\n",
    "    characters = (numbers+start).astype(np.int32).view('U1')\n",
    "    return \" \".join([\"\".join(t) for t in characters])\n",
    "\n",
    "def data_preparation(labeled_data):\n",
    "    \"\"\"Prepare data for training, validation, and testing.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    train_df : pd.DataFrame\n",
    "        Training data with 2 columns, \"text\" and \"label\".\n",
    "    val_df : pd.DataFrame\n",
    "        Validation data with 2 columns, \"text\" and \"label\".\n",
    "    test_df : pd.DataFrame\n",
    "        Testing data with 2 columns, \"text\" and \"label\".\n",
    "    \"\"\"\n",
    "\n",
    "    d = pd.read_pickle(labeled_data)\n",
    "    train_X, train_y, val_X, val_y, test_X, test_y = d[0], d[1], d[2], d[3], d[4], d[5]\n",
    "\n",
    "    train_df = pd.DataFrame({\"bscore\": train_X, \"label\": train_y})\n",
    "    val_df = pd.DataFrame({\"bscore\": val_X, \"label\": val_y})\n",
    "    test_df = pd.DataFrame({\"bscore\": test_X, \"label\": test_y})\n",
    "\n",
    "    train_df[\"text\"] = train_df[\"bscore\"].apply(dense_encoder)\n",
    "    val_df[\"text\"] = val_df[\"bscore\"].apply(dense_encoder)\n",
    "    test_df[\"text\"] = test_df[\"bscore\"].apply(dense_encoder)\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def label2id_function(examples, label2id):\n",
    "    return {\"label\": [label2id[label] for label in examples[\"label\"]]}\n",
    "\n",
    "def tokenizer_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone IMSLP bootleg scores\n",
    "repo_path = data_path/\"piano_bootleg_scores\"\n",
    "!git clone https://github.com/HMC-MIR/piano_bootleg_scores.git {repo_path}\n",
    "piano_bootleg_scores_path = repo_path/\"imslp_bootleg_dir-v1\"\n",
    "\n",
    "# Grab all file locations\n",
    "piece_names = glob.glob(str(Path(piano_bootleg_scores_path)/\"**/*\"))\n",
    "# This gets every version of every piece\n",
    "fnames = glob.glob(str(piano_bootleg_scores_path/\"**/*.pkl\"), recursive=True)\n",
    "\n",
    "\n",
    "# Filter out filler\n",
    "composer_dict = process_filler(\"../../filler.tsv\", piano_bootleg_scores_path, filler_threshold=0.5)\n",
    "\n",
    "# CONVERTING THE DATA TO BINARY MATRICES - MIGHT TAKE A MINUTE\n",
    "\n",
    "t0 = time.time()\n",
    "# List of tuples containing (binary_score, composer)\n",
    "pieces = []\n",
    "for composer in tqdm(composer_dict):\n",
    "    for piece in composer_dict[composer]:\n",
    "        for id in composer_dict[composer][piece]:\n",
    "            # Get the pages\n",
    "            pages = composer_dict[composer][piece][id][\"bscore\"]\n",
    "            # Convert them into binary matrices\n",
    "            bscores = [ints_to_binary_matrix(page) for page in pages]\n",
    "\n",
    "            if len(bscores) > 0:\n",
    "                p = np.concatenate(bscores, axis=0)\n",
    "                pieces.append((p, composer))\n",
    "\n",
    "print(\"Seconds to complete:\", round(time.time() - t0))\n",
    "\n",
    "# Split up data to train and valid\n",
    "train, valid, test = create_dataset(pieces, valid_split=.2, test_split=0)\n",
    "\n",
    "# Encode and save data LM pretraining data\n",
    "train_encoded = []\n",
    "for piece in tqdm(train):\n",
    "    train_encoded.append(dense_encoder(piece[0], block_size=[1,8]))\n",
    "valid_encoded = []\n",
    "for piece in tqdm(valid):\n",
    "    valid_encoded.append(dense_encoder(piece[0], block_size=[1,8]))\n",
    "\n",
    "train_file = gpt2_dir/\"LM_pretraining_data/train.txt\"\n",
    "train_file.parent.mkdir(exist_ok=True)\n",
    "valid_file = gpt2_dir/\"LM_pretraining_data/valid.txt\"\n",
    "valid_file.parent.mkdir(exist_ok=True)\n",
    "with open(train_file, \"w\") as f:\n",
    "    f.write(\"\\n\\n\".join(train_encoded))\n",
    "with open(valid_file, \"w\") as f:\n",
    "    f.write(\"\\n\\n\".join(valid_encoded))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30_000\n",
    "'''Vocab size for tokenizer: int'''\n",
    "special_tokens = []\n",
    "'''Special tokens for tokenizer: list of strings'''\n",
    "\n",
    "tokenizer_path = gpt2_dir/\"tokenizer\"/\"tokenizer.json\"\n",
    "tokenizer_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Define tokenizer\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.normalizer = Sequence([NFD(),Lowercase(),StripAccents()])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.decoder = BPEDecoder()\n",
    "trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "tokenizer.train([str(train_file)], trainer=trainer) \n",
    "tokenizer.save(str(tokenizer_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model Pretraining"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run the cell below and it will output an LM training shell script in the output model directory you specify. Navigate to that directory and run the shell script in a persistent shell session (TMUX) with the corresponding Python environment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language Model Config\n",
    "config_class = GPT2Config\n",
    "'''Config class for language model: e.g. GPT2Config'''\n",
    "\n",
    "lm_config = {\n",
    "    'model_type': 'gpt2', # e.g. 'gpt2',\n",
    "    'vocab_size': 30_000, # e.g. 50257,\n",
    "    'n_positions': 1024, # e.g. 1024,\n",
    "    'n_layer': 6, # e.g. 12,\n",
    "    # add more config here if needed\n",
    "}\n",
    "'''Config for language model: dict\n",
    "See https://huggingface.co/transformers/model_doc/gpt2.html#gpt2config for an example for GPT2'''\n",
    "\n",
    "lm_config_str = \"\\\"\" + \",\".join(f\"{k}={v}\" for k,v in lm_config.items()) + \"\\\"\"\n",
    "\n",
    "# Write out config to tokenizer directory for internal HuggingFace use\n",
    "with open(str(tokenizer_path.parent/\"config.json\"), 'w') as fp:\n",
    "    json.dump(lm_config, fp)\n",
    "\n",
    "\n",
    "# Create training shell script\n",
    "cmd = f''' \n",
    "torchrun  --nproc_per_node 2\n",
    "{Path.cwd()/\"run_clm.py\"}\n",
    "--model_type {lm_config[\"model_type\"]}\n",
    "--tokenizer_name {tokenizer_path.parent.resolve()}\n",
    "--train_file {train_file.resolve()}\n",
    "--validation_file {valid_file.resolve()}\n",
    "--output_dir {pretrained_output.resolve()}\n",
    "--do_train\n",
    "--do_eval\n",
    "--evaluation_strategy steps\n",
    "--per_device_train_batch_size 8\n",
    "--per_gpu_eval_batch_size 8\n",
    "--learning_rate 1e-4\n",
    "--num_train_epochs 12\n",
    "--logging_steps 2000\n",
    "--save_steps 2000\n",
    "--seed {seed}\n",
    "--overwrite_output_dir\n",
    "--local_rank 0\n",
    "--cache_dir {cache_dir}\n",
    "--config_overrides=\"{lm_config_str}\"\n",
    "'''.replace(\"\\n\", \" \")\n",
    "\n",
    "# Write out training shell script to output model directory\n",
    "with open(str(pretrained_output/\"train_lm.sh\"), \"w\") as fout:\n",
    "    fout.write(cmd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model Pretraining Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss = []\n",
    "val_loss = []\n",
    "accuracy = []\n",
    "step = []\n",
    "\n",
    "with open(pretrained_output/\"trainer_state.json\", 'r') as f:\n",
    "    log_history = json.load(f)['log_history']\n",
    "    for entry in log_history:\n",
    "        if \"loss\" in entry:\n",
    "            step.append(int(entry[\"step\"]))\n",
    "            tr_loss.append(float(entry[\"loss\"]))\n",
    "        elif \"eval_loss\" in entry:\n",
    "            val_loss.append(float(entry[\"eval_loss\"]))\n",
    "\n",
    "        if \"eval_accuracy\" in entry:\n",
    "            accuracy.append(float(entry[\"eval_accuracy\"]))\n",
    "\n",
    "step, tr_loss, val_loss, acc = np.array(step), np.array(tr_loss), np.array(val_loss), np.array(accuracy)\n",
    "plt.plot(step, tr_loss, 'k-', label=\"Train\")\n",
    "plt.scatter(step, tr_loss, c='k')\n",
    "plt.plot(step, val_loss, 'g-', label=\"Validation\")\n",
    "plt.scatter(step, val_loss, c='g')\n",
    "plt.legend()\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(step, acc, 'k-', label=\"Validation\")\n",
    "plt.scatter(step, acc, c='k')\n",
    "plt.legend()\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model Linear Probe Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prewritten Helper Functions & Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def top_x_acc(y_true, y_pred, x)\n",
    "#     y_true = torch.tensor(y_true)\n",
    "#     y_pred = torch.tensor(y_pred)\n",
    "#     ranked = torch.argsort(y_pred, axis=-1)\n",
    "#     top_x = ranked[..., -x:]\n",
    "#     return (top_x == torch.repeat_interleave(y_true.unsqueeze(-1), x, axis=-1)).float().sum(-1).mean().item()\n",
    "\n",
    "# def mean_recip_rank(y_true, y_pred):\n",
    "#     y_true = torch.tensor(y_true)\n",
    "#     y_pred = torch.tensor(y_pred)\n",
    "#     ranked = torch.argsort(y_pred, axis=-1)\n",
    "#     true_ranks = y_pred.shape[-1] - (ranked == torch.repeat_interleave(y_true.unsqueeze(-1), y_pred.shape[-1], axis=-1)).float().argmax(-1)\n",
    "\n",
    "#     return (1/true_ranks).mean().item()\n",
    "# def compute_metrics(eval_pred):\n",
    "#     print(eval_pred)\n",
    "\n",
    "#     predictions, labels = eval_pred\n",
    "#     # predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "#     top_one = top_x_acc(labels, predictions, 1)\n",
    "#     top_five = top_x_acc(labels, predictions, 5)\n",
    "#     mrr = mean_recip_rank(labels, predictions)\n",
    "\n",
    "#     metrics = {\"top_one\" : top_one, \"top_five\" : top_five, \"mrr\": mrr}\n",
    "#     return metrics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch_size = 32\n",
    "# '''Batch size for classifier training: int'''\n",
    "# epochs = 12\n",
    "# '''Epochs for classifier training: int'''\n",
    "# learning_rate = 5e-5\n",
    "# '''Learning rate for classifier training: float e.g. 5e-5'''\n",
    "\n",
    "# # Prepare data\n",
    "# # !unzip -o {labeled_data_path} -d {data_path} \n",
    "# labeled_data = data_path/\"9_way_dataset.pkl\"\n",
    "# train_df, val_df, test_df = data_preparation(labeled_data)\n",
    "# train_ds = Dataset.from_dict(train_df)\n",
    "# val_ds = Dataset.from_dict(val_df)\n",
    "# test_ds = Dataset.from_dict(test_df)\n",
    "\n",
    "# # Define label map\n",
    "# label2id = {label: i for i, label in enumerate(set(train_df['label']))}\n",
    "# id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# # Load Tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(pretrained_output)\n",
    "# tokenizer.pad_token = '<pad>'\n",
    "\n",
    "\n",
    "# # Define model\n",
    "# config = AutoConfig.from_pretrained(pretrained_output)\n",
    "# config.num_labels = len(label2id)\n",
    "# config.pad_token_id = tokenizer.pad_token_id\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(pretrained_output, config=config)\n",
    "# tokenizer.model_max_length = config.n_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Tokenize and convert labels to ids\n",
    "# train_ds = train_ds.map(tokenizer_function, batched=True, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "# val_ds = val_ds.map(tokenizer_function, batched=True, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "# train_ds = train_ds.map(label2id_function, batched=True, fn_kwargs={\"label2id\": label2id})\n",
    "# val_ds = val_ds.map(label2id_function, batched=True, fn_kwargs={\"label2id\": label2id})\n",
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\", max_length=1024)\n",
    "\n",
    "# # Freeze all layers except the classifier\n",
    "# for name, param in model.named_parameters():\n",
    "#     param.requires_grad = False\n",
    "# model.score.weight.requires_grad = True\n",
    "\n",
    "\n",
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=classifier_output_dir,\n",
    "#     learning_rate=learning_rate,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     num_train_epochs=epochs,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_strategy=\"epoch\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "#     push_to_hub=False,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_ds,\n",
    "#     eval_dataset=val_ds,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "# # Train model\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with open(\"/home/abunn/ttmp/ExplorationWithLLMs/source/03_alec_temp_name/finetuned_models/dense_1_8/log_history.json\", \"r\") as fin:\n",
    "#     log_history = json.load(fin)\n",
    "\n",
    "# step, tr_loss, val_loss = [], [], []\n",
    "# acc = []\n",
    "# for epoch in log_history:\n",
    "#     step.append(epoch[\"epoch\"])\n",
    "#     tr_loss.append(epoch[\"train_loss\"])\n",
    "#     val_loss.append(epoch[\"val_loss\"])\n",
    "#     acc.append(epoch[\"accuracy\"])\n",
    "\n",
    "# step, tr_loss, val_loss = np.array(step), np.array(tr_loss), np.array(val_loss)\n",
    "# plt.plot(step, tr_loss, 'k-', label=\"Train\")\n",
    "# plt.scatter(step, tr_loss, c='k')\n",
    "# plt.plot(step, val_loss, 'g-', label=\"Validation\")\n",
    "# plt.scatter(step, val_loss, c='g')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Steps')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(step, acc, 'g-', label=\"Validation\")\n",
    "# plt.scatter(step, acc, c='g')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Steps')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
