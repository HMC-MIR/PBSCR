{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "\n",
    "# import encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the path to the labeled dataset\n",
    "data_path = Path(\"/home/ajain/ttmp/PBSCSR_data/\")\n",
    "repo_path = data_path/\"piano_bootleg_scores\"\n",
    "piano_bootleg_scores_path = repo_path/\"imslp_bootleg_dir-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31834\n"
     ]
    }
   ],
   "source": [
    "# Grab all file locations\n",
    "piece_names = glob.glob(str(Path(piano_bootleg_scores_path)/\"**/*\"))\n",
    "\n",
    "# This gets only one version of each piece\n",
    "# fnames = [glob.glob(str(Path(piece_name)/\"*.pkl\"))[0] for piece_name in piece_names if len(glob.glob(str(Path(piece_name)/\"*.pkl\"))) != 0]\n",
    "\n",
    "# This gets every version of every piece\n",
    "fnames = glob.glob(str(piano_bootleg_scores_path/\"**/*.pkl\"), recursive=True)\n",
    "\n",
    "print(len(fnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filler_file = \"../../filler.tsv\"\n",
    "filler = {}\n",
    "with open(filler_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip(\"\\n\").split(\"\\t\")\n",
    "        if not parts[0] in filler:\n",
    "            filler[parts[0]] = []\n",
    "        if float(parts[2]) >= 0.5:\n",
    "            filler[parts[0]].append(int(parts[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31834/31834 [05:25<00:00, 97.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds to complete: 326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## HELPER\n",
    "def ints_to_binary_matrix(score_seq):  # converts integer sequence to n x 62 matrix\n",
    "    matrix = []\n",
    "    for event in score_seq:\n",
    "        binary_rep = list(np.binary_repr(event, 62))\n",
    "        matrix.append(binary_rep)\n",
    "    np_mat = np.array(matrix, dtype=np.uint8)\n",
    "    #np_mat = np.flip(np_mat, axis=0)  # flip to have least significant bit at the front\n",
    "    return np_mat\n",
    "# CONVERTING THE DATA TO BINARY MATRICES - MIGHT TAKE A MINUTE\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "# List of tuples\n",
    "# Tuples contain (binary_score, composer)\n",
    "pieces = []\n",
    "\n",
    "for fname in tqdm(fnames):\n",
    "    # Load the pages\n",
    "    pages = pd.read_pickle(fname)\n",
    "\n",
    "    filler_key = fname.split(\"imslp_bootleg_dir-v1/\")[1].strip(\".pkl\")\n",
    "\n",
    "    filler_pages = filler[filler_key] if filler_key in filler.keys() else []\n",
    "\n",
    "    # Convert them into binary matrices\n",
    "    bscores = [ints_to_binary_matrix(page) for i, page in enumerate(pages) if i not in filler_pages]\n",
    "    bscores = [page for page in bscores if len(page.shape) == 2 and page.shape[1] == 62]\n",
    "\n",
    "    # If there were binary scores, then combine them into one and append to dataset.\n",
    "    if len(bscores) > 0:\n",
    "        piece = np.concatenate(bscores, axis=0)\n",
    "        pieces.append(piece)\n",
    "\n",
    "print(\"Seconds to complete:\", round(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1738852\n"
     ]
    }
   ],
   "source": [
    "print(sum(len(piece[0]) for piece in pieces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(pieces, valid_split=.15, test_split=.15):\n",
    "    \"\"\"\n",
    "    Creates a train / valid / test split dataset of pieces.\n",
    "    pieces: The list of binary_matrices to sample from\n",
    "    valid_split: The proportion of data to use for valid\n",
    "    test_split: The proportion of data to use for valid\n",
    "    \n",
    "    returns:\n",
    "    x & y lists for train, valid, and test sets\n",
    "    \"\"\"\n",
    "    \n",
    "    # For repeatability\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # shuffle pieces\n",
    "    piece_list = [piece for piece in pieces]\n",
    "    np.random.shuffle(piece_list)\n",
    "    \n",
    "    # Calculate starting places of each section - order is (test, valid, train)\n",
    "    train_start = round((valid_split+test_split)*len(piece_list))\n",
    "    valid_start = round(test_split*len(piece_list))\n",
    "    \n",
    "    # Go through and separate pieces into train, valid, test\n",
    "    train_pieces = piece_list[train_start:]\n",
    "    valid_pieces = piece_list[valid_start:train_start]\n",
    "    test_pieces = piece_list[:valid_start]\n",
    "    \n",
    "    return train_pieces, valid_pieces, test_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = create_dataset(pieces, valid_split=.2, test_split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22437\n",
      "5609\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Encoder\n",
    "\n",
    "# Continuous line of 256 unicode characters\n",
    "start = 10060# 931\n",
    "dense_characters = [chr(i).encode(\"utf-8\").decode(\"utf-8\") for i in range(start, start+512)]\n",
    "\n",
    "\n",
    "# This code divides the fragment into blocks (and discards any remaining info at the very edges)\n",
    "# Then it uses einsum with a filter of powers of 2 to convert from binary to an integer.  Then converts integers into\n",
    "# unicode characters\n",
    "\n",
    "def merge_staff_overlaps(bscores):\n",
    "    \"\"\"\n",
    "    Takes in either one binary score or a batch of them and merges the left and right hands\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lower middle c is index 23\n",
    "    # Upper middle c is index 33\n",
    "    lower = 23\n",
    "    upper = 33\n",
    "    middle = (lower + upper) // 2\n",
    "    \n",
    "    # Total notes is 52\n",
    "    total = 52\n",
    "    \n",
    "    # Pad out upper hand and lower hand and combine them\n",
    "    padded_lower = np.concatenate([bscores[..., :middle], np.zeros((*bscores.shape[:-1], total-middle))], axis=-1)\n",
    "    padded_upper = np.concatenate([np.zeros((*bscores.shape[:-1], middle-bscores.shape[-1]+total)), bscores[..., middle:]], axis=-1)\n",
    "    # Logical or\n",
    "    merged = padded_lower + padded_upper - padded_lower * padded_upper\n",
    "    return merged\n",
    "\n",
    "def dense_encoder(fragment, block_size=[1, 1]):\n",
    "    fragment = merge_staff_overlaps(fragment)\n",
    "    # Rewrote this to be much faster but looks complicated\n",
    "    # This filter has powers of 2 which is how the binary is turned to ints\n",
    "    filter_ = np.power(2, np.arange(np.prod(block_size))).reshape(block_size)\n",
    "    \n",
    "    # The fragment is split into blocks here\n",
    "    xblocks = np.stack(np.split(fragment[:, :(fragment.shape[1]//block_size[1])*block_size[1]], fragment.shape[1]//block_size[1], axis=1))\n",
    "    xyblocks = np.stack(np.split(xblocks[:, :(xblocks.shape[1]//block_size[0])*block_size[0]], xblocks.shape[1]//block_size[0], axis=1))\n",
    "    \n",
    "    # The blocks are multiplied so they are ints\n",
    "    numbers = np.einsum(\"ijkl,kl->ij\", xyblocks, filter_)\n",
    "    \n",
    "    # The ints are turned into corresponding characters\n",
    "    characters = (numbers+start).astype(np.int32).view('U1')\n",
    "    return \" \".join([\"\".join(t) for t in characters])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_encoded = []\n",
    "for piece in tqdm(train):\n",
    "    train_encoded.append(dense_encoder(piece, block_size=[1,8]))\n",
    "    \n",
    "valid_encoded = []\n",
    "for piece in tqdm(valid):\n",
    "    valid_encoded.append(dense_encoder(piece, block_size=[1,8]))\n",
    "    \n",
    "# Data for LM pretraining\n",
    "lm_pretraining_dir = data_path/\"LM_pretraining_data\"\n",
    "lm_pretraining_dir.mkdir(exist_ok=True)\n",
    "with open(data_path/\"LM_pretraining_data/dense_1_8-train.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\\n\".join(train_encoded))\n",
    "with open(data_path/\"LM_pretraining_data/dense_1_8-valid.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\\n\".join(valid_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EWLLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
