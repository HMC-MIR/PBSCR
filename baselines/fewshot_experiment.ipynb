{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../9_way_dataset.pkl\", \"rb\") as f:\n",
    "    x_train9, y_train9, x_valid9, y_valid9, x_test9, y_test9, m_train9, m_valid9, m_test9 = pickle.load(f)\n",
    "\n",
    "with open(f\"../100_way_dataset.pkl\", \"rb\") as f:\n",
    "    x_train100, y_train100, x_valid100, y_valid100, x_test100, y_test100, m_train100, m_valid100, m_test100 = pickle.load(f)\n",
    "\n",
    "composers9 = np.unique(y_train9)\n",
    "composers100 = np.unique(y_train100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(y_train, composers, N=10):\n",
    "    composer_pieces = {composer:np.argwhere(y_train == composer).flatten() for composer in composers}\n",
    "    indices = np.concatenate([np.random.choice(piece_indices, size=(N,), replace=False) for piece_indices in composer_pieces.values()])\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Helper funcs\n",
    "def expand_repeat(arr, repeats, axis=0):\n",
    "    return np.repeat(np.expand_dims(arr, axis=axis), repeats, axis=axis)\n",
    "\n",
    "def fit_and_predict(train_features, test_features, indices, y_train, composers, k=3): # Function to take x_few and y_few and make guesses about all the x_test samples\n",
    "\n",
    "    avg_euclideans = np.zeros((len(test_features), len(composers)))\n",
    "    few_features = train_features[indices]# L x d (where L is N*#composers)\n",
    "    batch_size = 128\n",
    "\n",
    "    for i, composer in enumerate(composers):\n",
    "        composer_points = np.argwhere(y_train[indices] == composer).flatten()\n",
    "        euclideans = np.zeros((len(test_features), len(composer_points)))\n",
    "\n",
    "        for batch in range(0, int(batch_size*np.ceil(len(test_features)/batch_size)), batch_size):\n",
    "            test_batch = test_features[batch:batch+batch_size]\n",
    "            euclideans[batch:batch+batch_size] = np.sqrt(np.sum(np.square(expand_repeat(few_features[composer_points], len(test_batch), 0) - expand_repeat(test_batch, len(composer_points), 1)), axis=-1))\n",
    "\n",
    "        \n",
    "        avg_euclideans[:, i] = np.mean(np.sort(euclideans, axis=-1)[..., :k], axis=-1)\n",
    "\n",
    "    ranks = np.argsort(avg_euclideans, axis=-1)\n",
    "\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_x_acc(y_true, y_pred, x):\n",
    "    y_true = torch.Tensor(y_true)\n",
    "    y_pred = torch.Tensor(y_pred)\n",
    "    \n",
    "    top_x = y_pred[..., :x]\n",
    "    return (top_x == torch.repeat_interleave(y_true.unsqueeze(-1), x, axis=-1)).float().sum(-1).mean().item()\n",
    "\n",
    "def mean_recip_rank(y_true, y_pred):\n",
    "    y_true = torch.Tensor(y_true)\n",
    "    y_pred = torch.Tensor(y_pred)\n",
    "    \n",
    "    # starts with worst at 0 but we want best at 1 so\n",
    "    true_ranks = (y_pred == torch.repeat_interleave(y_true.unsqueeze(-1), y_pred.shape[-1], axis=-1)).float().argmax(-1) + 1\n",
    "\n",
    "    return (1/true_ranks).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Way dataset:\n",
      "\n",
      "N: 1\n",
      "\n",
      "\n",
      "Model: gpt2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c459a0af3b347d893ebb18de76fd071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'top1': (0.15373979806900023, 0.021686374132203746), 'top5': (0.6099339246749877, 0.024527258531666), 'top10': None, 'mrr': (0.35770878394444783, 0.017534150391536333)}\n",
      "\n",
      "Model: roberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5da1c3efbe245e69050df1794602cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'top1': (0.145965904990832, 0.014984276242118318), 'top5': (0.6197789967060089, 0.024101709686708935), 'top10': None, 'mrr': (0.3543575425942739, 0.013889564367320036)}\n",
      "\n",
      "Model: random\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d623c7a4c774311bd014e4fdb2f45d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'top1': (0.11184407795468966, 0.00375646971936905), 'top5': (0.5530456999937693, 0.005043187391147398), 'top10': None, 'mrr': (0.3144984116156896, 0.003002049472961737)}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "models = [\"gpt2\", \"roberta\", \"random\"]\n",
    "T = 30\n",
    "Ns = [1]# , 10, 100]\n",
    "datasets = {\n",
    "    \"9\":(y_train9, y_test9, composers9), \n",
    "    # \"100\":(y_train100, y_test100, composers100)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for ways, (y_train, y_test, composers) in datasets.items():\n",
    "    print(f\"{ways} Way dataset:\")\n",
    "\n",
    "    results[ways] = {}\n",
    "\n",
    "    for N in Ns:\n",
    "        results[ways][N] = {}\n",
    "        print()\n",
    "        print(f\"N: {N}\")\n",
    "        print()\n",
    "\n",
    "        for model in models:\n",
    "            train_features, test_features = np.load(f\"fewshot_vecs/{model}_train{ways}.npy\"), np.load(f\"fewshot_vecs/{model}_test{ways}.npy\")\n",
    "\n",
    "            print()\n",
    "            print(f\"Model: {model}\")\n",
    "\n",
    "            top1s = []\n",
    "            top5s = []\n",
    "            mrrs = []\n",
    "            top10s = []\n",
    "\n",
    "            for _ in tqdm(range(T)):\n",
    "                indices = get_indices(y_train, composers, N=N)\n",
    "\n",
    "                y_pred = fit_and_predict(train_features, test_features, indices, y_train, composers, k=3)\n",
    "                y_true = np.array([list(composers).index(i) for i in y_test])\n",
    "\n",
    "                top1s.append(top_x_acc(y_true, y_pred, 1))\n",
    "                top5s.append(top_x_acc(y_true, y_pred, 5))\n",
    "                mrrs.append(mean_recip_rank(y_true, y_pred))\n",
    "\n",
    "                if ways == \"100\":\n",
    "                    top10s.append(top_x_acc(y_true, y_pred, 10))\n",
    "\n",
    "            results[ways][N][model] = {\"top1\":(np.mean(top1s), np.std(top1s)), \"top5\":(np.mean(top5s), np.std(top5s)), \"top10\":(np.mean(top10s), np.std(top10s)) if top10s else None, \"mrr\":(np.mean(mrrs), np.std(mrrs))}\n",
    "            print(results[ways][N][model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "not readable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/abunn/ttmp/PBSCSR/baselines/fewshot_experiment.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmirlab5.cs.hmc.edu/home/abunn/ttmp/PBSCSR/baselines/fewshot_experiment.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mresults.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmirlab5.cs.hmc.edu/home/abunn/ttmp/PBSCSR/baselines/fewshot_experiment.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     results \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mload(f)\n",
      "File \u001b[0;32m~/ttmp/miniconda3/envs/EWLLMs/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(fp, \u001b[39m*\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_float\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_pairs_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m     \u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39;49mread(),\n\u001b[1;32m    294\u001b[0m         \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m, object_hook\u001b[39m=\u001b[39mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[39m=\u001b[39mparse_float, parse_int\u001b[39m=\u001b[39mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[39m=\u001b[39mparse_constant, object_pairs_hook\u001b[39m=\u001b[39mobject_pairs_hook, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "\u001b[0;31mUnsupportedOperation\u001b[0m: not readable"
     ]
    }
   ],
   "source": [
    "with open(\"results.json\", \"r\") as f:\n",
    "    results = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EWLLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
