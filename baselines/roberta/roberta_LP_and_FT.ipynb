{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import time\n",
    "import evaluate\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.normalizers import (Sequence, Lowercase, NFD, StripAccents)\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.decoders import BPEDecoder\n",
    "from transformers import AutoConfig, \\\n",
    "    DataCollatorWithPadding, AutoModelForSequenceClassification, \\\n",
    "    Trainer, TrainingArguments, AutoTokenizer, RobertaConfig, RobertaModel, RobertaForSequenceClassification, RobertaTokenizer\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from transformers import get_scheduler, AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_x_acc(y_true, y_pred, x):\n",
    "    y_true = torch.tensor(y_true)\n",
    "    y_pred = torch.tensor(y_pred)\n",
    "    ranked = torch.argsort(y_pred, axis=-1)\n",
    "    top_x = ranked[..., -x:]\n",
    "    return (top_x == torch.repeat_interleave(y_true.unsqueeze(-1), x, axis=-1)).float().sum(-1).mean().item()\n",
    "\n",
    "def mean_recip_rank(y_true, y_pred):\n",
    "    y_true = torch.tensor(y_true)\n",
    "    y_pred = torch.tensor(y_pred)\n",
    "    ranked = torch.argsort(y_pred, axis=-1)\n",
    "    true_ranks = y_pred.shape[-1] - (ranked == torch.repeat_interleave(y_true.unsqueeze(-1), y_pred.shape[-1], axis=-1)).float().argmax(-1)\n",
    "\n",
    "    return (1/true_ranks).mean().item()\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    top_one = top_x_acc(labels, predictions, 1)\n",
    "    top_five = top_x_acc(labels, predictions, 5)\n",
    "    top_ten = 0\n",
    "    if predictions.shape[-1] >= 10:\n",
    "        top_ten = top_x_acc(labels, predictions, 10)\n",
    "    mrr = mean_recip_rank(labels, predictions)\n",
    "\n",
    "    metrics = {\"top_one\" : top_one, \"top_five\" : top_five, \"mrr\": mrr, \"top_ten\": top_ten}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_staff_overlaps(bscores):\n",
    "    \"\"\"\n",
    "    Takes in either one binary score or a batch of them and merges the left and right hands\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lower middle c is index 23\n",
    "    # Upper middle c is index 33\n",
    "    lower = 23\n",
    "    upper = 33\n",
    "    middle = (lower + upper) // 2\n",
    "    \n",
    "    # Total notes is 52\n",
    "    total = 52\n",
    "    \n",
    "    # Pad out upper hand and lower hand and combine them\n",
    "    padded_lower = np.concatenate([bscores[..., :middle], np.zeros((*bscores.shape[:-1], total-middle))], axis=-1)\n",
    "    padded_upper = np.concatenate([np.zeros((*bscores.shape[:-1], middle-bscores.shape[-1]+total)), bscores[..., middle:]], axis=-1)\n",
    "    # Logical or\n",
    "    merged = padded_lower + padded_upper - padded_lower * padded_upper\n",
    "    return merged\n",
    "\n",
    "\n",
    "# Continuous line of 256 unicode characters\n",
    "start = 10060# 931\n",
    "dense_characters = [chr(i).encode(\"utf-8\").decode(\"utf-8\") for i in range(start, start+512)]\n",
    "\n",
    "\n",
    "# This code divides the fragment into blocks (and discards any remaining info at the very edges)\n",
    "# Then it uses einsum with a filter of powers of 2 to convert from binary to an integer.  Then converts integers into\n",
    "# unicode characters\n",
    "\n",
    "def dense_encoder(fragment, block_size=[1, 1]):\n",
    "    fragment = merge_staff_overlaps(fragment)\n",
    "    # Rewrote this to be much faster but looks complicated\n",
    "    # This filter has powers of 2 which is how the binary is turned to ints\n",
    "    filter_ = np.power(2, np.arange(np.prod(block_size))).reshape(block_size)\n",
    "    \n",
    "    # The fragment is split into blocks here\n",
    "    xblocks = np.stack(np.split(fragment[:, :(fragment.shape[1]//block_size[1])*block_size[1]], fragment.shape[1]//block_size[1], axis=1))\n",
    "    xyblocks = np.stack(np.split(xblocks[:, :(xblocks.shape[1]//block_size[0])*block_size[0]], xblocks.shape[1]//block_size[0], axis=1))\n",
    "    \n",
    "    # The blocks are multiplied so they are ints\n",
    "    numbers = np.einsum(\"ijkl,kl->ij\", xyblocks, filter_)\n",
    "    \n",
    "    # The ints are turned into corresponding characters\n",
    "    characters = (numbers+start).astype(np.int32).view('U1')\n",
    "    return \" \".join([\"\".join(t) for t in characters])\n",
    "\n",
    "def data_preparation(labeled_data):\n",
    "    train_X, train_y, val_X, val_y, test_X, test_y, train_m, valid_m, test_m = pd.read_pickle(labeled_data)\n",
    "\n",
    "    train_df = pd.DataFrame({\"text\": [dense_encoder(piece, block_size=[1,8]) for piece in train_X], \"label\": train_y})\n",
    "    val_df = pd.DataFrame({\"text\": [dense_encoder(piece, block_size=[1,8]) for piece in val_X], \"label\": val_y})\n",
    "    test_df = pd.DataFrame({\"text\": [dense_encoder(piece, block_size=[1,8]) for piece in test_X], \"label\": test_y})\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def label2id_function(examples, label2id):\n",
    "    return {\"label\": [label2id[label] for label in examples[\"label\"]]}\n",
    "\n",
    "def tokenizer_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# gpt2_dir = Path(\"./\")\n",
    "roberta_dir = Path(\"/home/jliu/ttmp/PBSCSR/roberta\")\n",
    "seed = 42\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(gpt2_dir/\"roberta_pretrained\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta_dir/\"tokenizer\")\n",
    "tokenizer.pad_token = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08f8c460d7441278bdbe88eff38f19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5608122c90014c5593b969cb8b86b5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13131a3769dd4a86916a79862371e833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f330a104da046e0bb42deb9819304cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052207c683e048a392d80adafd25a91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a38b1bd8ff40bcbe6b5339d23100ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# labeled_data = \"/home/jliu/ttmp/PBSCSR/baselines/9_way_dataset.pkl\"\n",
    "labeled_data = \"/home/jliu/ttmp/PBSCSR/baselines/100_way_dataset.pkl\"\n",
    "\n",
    "train_df, val_df, test_df = data_preparation(labeled_data)\n",
    "train_ds = Dataset.from_dict(train_df)\n",
    "val_ds = Dataset.from_dict(val_df)\n",
    "test_ds = Dataset.from_dict(test_df)\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(set(train_df['label']))}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "train_ds = train_ds.map(tokenizer_function, batched=True, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "train_ds = train_ds.map(label2id_function, batched=True, fn_kwargs={\"label2id\": label2id})\n",
    "val_ds = val_ds.map(label2id_function, batched=True, fn_kwargs={\"label2id\": label2id})\n",
    "val_ds = val_ds.map(tokenizer_function, batched=True, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "test_ds = test_ds.map(tokenizer_function, batched=True, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "test_ds = test_ds.map(label2id_function, batched=True, fn_kwargs={\"label2id\": label2id})\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\", max_length=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(roberta_dir/\"pretrained_model\")\n",
    "config.num_labels = len(label2id)\n",
    "config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_config(config=config)\n",
    "\n",
    "\n",
    "# Freeze all layers except the classifier\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "model.classifier.out_proj.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13128' max='13128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13128/13128 18:55, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Top One</th>\n",
       "      <th>Top Five</th>\n",
       "      <th>Mrr</th>\n",
       "      <th>Top Ten</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.625000</td>\n",
       "      <td>4.611366</td>\n",
       "      <td>0.011333</td>\n",
       "      <td>0.054133</td>\n",
       "      <td>0.054186</td>\n",
       "      <td>0.106200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.614800</td>\n",
       "      <td>4.607971</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>0.056202</td>\n",
       "      <td>0.107133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.607300</td>\n",
       "      <td>4.604663</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.056829</td>\n",
       "      <td>0.113667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.600200</td>\n",
       "      <td>4.602517</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.061467</td>\n",
       "      <td>0.059111</td>\n",
       "      <td>0.117467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.595300</td>\n",
       "      <td>4.600651</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.059820</td>\n",
       "      <td>0.120667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.591400</td>\n",
       "      <td>4.599602</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.063533</td>\n",
       "      <td>0.061235</td>\n",
       "      <td>0.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.587000</td>\n",
       "      <td>4.598385</td>\n",
       "      <td>0.013267</td>\n",
       "      <td>0.063133</td>\n",
       "      <td>0.060565</td>\n",
       "      <td>0.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.583600</td>\n",
       "      <td>4.597538</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.066933</td>\n",
       "      <td>0.061871</td>\n",
       "      <td>0.123067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.580800</td>\n",
       "      <td>4.596832</td>\n",
       "      <td>0.014267</td>\n",
       "      <td>0.066267</td>\n",
       "      <td>0.061932</td>\n",
       "      <td>0.123600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.578800</td>\n",
       "      <td>4.596418</td>\n",
       "      <td>0.014733</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>0.062137</td>\n",
       "      <td>0.123267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.577100</td>\n",
       "      <td>4.596166</td>\n",
       "      <td>0.015067</td>\n",
       "      <td>0.066867</td>\n",
       "      <td>0.062583</td>\n",
       "      <td>0.123133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.576600</td>\n",
       "      <td>4.596110</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>0.062182</td>\n",
       "      <td>0.123733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13128, training_loss=4.5931636012958945, metrics={'train_runtime': 1139.1225, 'train_samples_per_second': 737.41, 'train_steps_per_second': 11.525, 'total_flos': 1.847751883061376e+16, 'train_loss': 4.5931636012958945, 'epoch': 12.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    # output_dir=roberta_dir/\"classifier_9_no_pretrained\",\n",
    "    output_dir=roberta_dir/\"classifier_100_no_pretrained\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=12,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94/94 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.178577423095703,\n",
       " 'eval_top_one': 0.14875894784927368,\n",
       " 'eval_top_five': 0.6271864175796509,\n",
       " 'eval_mrr': 0.3593359887599945,\n",
       " 'eval_top_ten': 0,\n",
       " 'eval_runtime': 10.0796,\n",
       " 'eval_samples_per_second': 595.561,\n",
       " 'eval_steps_per_second': 9.326,\n",
       " 'epoch': 12.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /home/jliu/ttmp/PBSCSR/roberta/pretrained_model and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(roberta_dir/\"pretrained_model\")\n",
    "config.num_labels = len(label2id)\n",
    "config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta_dir/\"pretrained_model\", config=config)\n",
    "\n",
    "\n",
    "# Freeze all layers except the classifier\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "model.classifier.out_proj.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5256' max='5256' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5256/5256 24:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Top One</th>\n",
       "      <th>Top Five</th>\n",
       "      <th>Mrr</th>\n",
       "      <th>Top Ten</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.181300</td>\n",
       "      <td>2.135515</td>\n",
       "      <td>0.190571</td>\n",
       "      <td>0.676495</td>\n",
       "      <td>0.397929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.109200</td>\n",
       "      <td>2.073681</td>\n",
       "      <td>0.229718</td>\n",
       "      <td>0.742962</td>\n",
       "      <td>0.440324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.060800</td>\n",
       "      <td>2.031308</td>\n",
       "      <td>0.253373</td>\n",
       "      <td>0.769948</td>\n",
       "      <td>0.463606</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.024800</td>\n",
       "      <td>2.000467</td>\n",
       "      <td>0.263701</td>\n",
       "      <td>0.787273</td>\n",
       "      <td>0.476693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.000400</td>\n",
       "      <td>1.977895</td>\n",
       "      <td>0.276862</td>\n",
       "      <td>0.799267</td>\n",
       "      <td>0.487733</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.981600</td>\n",
       "      <td>1.960823</td>\n",
       "      <td>0.286024</td>\n",
       "      <td>0.808263</td>\n",
       "      <td>0.495658</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.965100</td>\n",
       "      <td>1.947851</td>\n",
       "      <td>0.290188</td>\n",
       "      <td>0.812594</td>\n",
       "      <td>0.499755</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.954200</td>\n",
       "      <td>1.938177</td>\n",
       "      <td>0.293853</td>\n",
       "      <td>0.819757</td>\n",
       "      <td>0.503565</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.947700</td>\n",
       "      <td>1.931075</td>\n",
       "      <td>0.296518</td>\n",
       "      <td>0.820257</td>\n",
       "      <td>0.505901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.941700</td>\n",
       "      <td>1.926272</td>\n",
       "      <td>0.298684</td>\n",
       "      <td>0.822589</td>\n",
       "      <td>0.507991</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.936800</td>\n",
       "      <td>1.923473</td>\n",
       "      <td>0.299350</td>\n",
       "      <td>0.824421</td>\n",
       "      <td>0.509051</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.936500</td>\n",
       "      <td>1.922579</td>\n",
       "      <td>0.300350</td>\n",
       "      <td>0.824754</td>\n",
       "      <td>0.509574</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5256, training_loss=2.0033310145547945, metrics={'train_runtime': 1448.6131, 'train_samples_per_second': 231.938, 'train_steps_per_second': 3.628, 'total_flos': 7514471469594078.0, 'train_loss': 2.0033310145547945, 'epoch': 12.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=roberta_dir/\"classifier_9_pretrained\",\n",
    "    # output_dir=roberta_dir/\"classifier_100_pretrained\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=12,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5470' max='5470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5470/5470 11:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Top One</th>\n",
       "      <th>Top Five</th>\n",
       "      <th>Mrr</th>\n",
       "      <th>Top Ten</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.222800</td>\n",
       "      <td>4.205184</td>\n",
       "      <td>0.078933</td>\n",
       "      <td>0.229333</td>\n",
       "      <td>0.168945</td>\n",
       "      <td>0.350267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.078700</td>\n",
       "      <td>4.115465</td>\n",
       "      <td>0.086333</td>\n",
       "      <td>0.249400</td>\n",
       "      <td>0.182187</td>\n",
       "      <td>0.379467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.996800</td>\n",
       "      <td>4.060020</td>\n",
       "      <td>0.090133</td>\n",
       "      <td>0.263333</td>\n",
       "      <td>0.190049</td>\n",
       "      <td>0.394867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.942500</td>\n",
       "      <td>4.024141</td>\n",
       "      <td>0.094933</td>\n",
       "      <td>0.270933</td>\n",
       "      <td>0.195992</td>\n",
       "      <td>0.402800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.900300</td>\n",
       "      <td>3.997250</td>\n",
       "      <td>0.098933</td>\n",
       "      <td>0.277733</td>\n",
       "      <td>0.201352</td>\n",
       "      <td>0.413533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='235' max='235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [235/235 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.011227607727051, 'eval_top_one': 0.09286666661500931, 'eval_top_five': 0.2701333463191986, 'eval_mrr': 0.19485780596733093, 'eval_top_ten': 0.4010666608810425, 'eval_runtime': 10.8309, 'eval_samples_per_second': 1384.923, 'eval_steps_per_second': 21.697, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5470' max='5470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5470/5470 11:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Top One</th>\n",
       "      <th>Top Five</th>\n",
       "      <th>Mrr</th>\n",
       "      <th>Top Ten</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.295600</td>\n",
       "      <td>4.320817</td>\n",
       "      <td>0.062467</td>\n",
       "      <td>0.187933</td>\n",
       "      <td>0.143520</td>\n",
       "      <td>0.296333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.215100</td>\n",
       "      <td>4.255294</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>0.213467</td>\n",
       "      <td>0.159878</td>\n",
       "      <td>0.327467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.158600</td>\n",
       "      <td>4.212419</td>\n",
       "      <td>0.078200</td>\n",
       "      <td>0.227733</td>\n",
       "      <td>0.167998</td>\n",
       "      <td>0.347733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.123200</td>\n",
       "      <td>4.184475</td>\n",
       "      <td>0.080733</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>0.172202</td>\n",
       "      <td>0.357467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.097600</td>\n",
       "      <td>4.165159</td>\n",
       "      <td>0.081933</td>\n",
       "      <td>0.238067</td>\n",
       "      <td>0.174734</td>\n",
       "      <td>0.364867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='235' max='235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [235/235 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.175278186798096, 'eval_top_one': 0.07813332974910736, 'eval_top_five': 0.2321999967098236, 'eval_mrr': 0.17059360444545746, 'eval_top_ten': 0.3547999858856201, 'eval_runtime': 11.4251, 'eval_samples_per_second': 1312.894, 'eval_steps_per_second': 20.569, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5470' max='5470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5470/5470 11:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Top One</th>\n",
       "      <th>Top Five</th>\n",
       "      <th>Mrr</th>\n",
       "      <th>Top Ten</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.329600</td>\n",
       "      <td>4.382401</td>\n",
       "      <td>0.054933</td>\n",
       "      <td>0.166200</td>\n",
       "      <td>0.129046</td>\n",
       "      <td>0.268667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.299900</td>\n",
       "      <td>4.355662</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.176467</td>\n",
       "      <td>0.135358</td>\n",
       "      <td>0.280600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.273200</td>\n",
       "      <td>4.332899</td>\n",
       "      <td>0.061333</td>\n",
       "      <td>0.184200</td>\n",
       "      <td>0.141355</td>\n",
       "      <td>0.290267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.253700</td>\n",
       "      <td>4.315032</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.189667</td>\n",
       "      <td>0.145546</td>\n",
       "      <td>0.299267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.236900</td>\n",
       "      <td>4.301610</td>\n",
       "      <td>0.066067</td>\n",
       "      <td>0.194533</td>\n",
       "      <td>0.148540</td>\n",
       "      <td>0.305133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jliu/ttmp/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='235' max='235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [235/235 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.307337284088135, 'eval_top_one': 0.060733333230018616, 'eval_top_five': 0.18913333117961884, 'eval_mrr': 0.1434733271598816, 'eval_top_ten': 0.30033332109451294, 'eval_runtime': 11.0733, 'eval_samples_per_second': 1354.611, 'eval_steps_per_second': 21.222, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "for lr in [5e-5/100, 5e-5/300, 5e-5/1000]:\n",
    "    os.makedirs(roberta_dir/f\"LPFT_100_{lr}\", exist_ok=True)\n",
    "    # os.makedirs(roberta_dir/f\"LPFT_9_{lr}\", exist_ok=True)\n",
    "    \n",
    "    # config = AutoConfig.from_pretrained(roberta_dir/\"classifier_9_pretrained/checkpoint-5256\")\n",
    "    config = AutoConfig.from_pretrained(roberta_dir/\"classifier_100_pretrained/checkpoint-13128\")\n",
    "    \n",
    "    config.num_labels = len(label2id)\n",
    "    config.pad_token_id = tokenizer.pad_token_id\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(roberta_dir/\"classifier_100_pretrained/checkpoint-13128\", config=config)\n",
    "    # model = AutoModelForSequenceClassification.from_pretrained(roberta_dir/\"classifier_9_pretrained/checkpoint-5256\", config=config)\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "    )\n",
    "\n",
    "    num_training_steps = len(train_ds) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "    scheduler = get_scheduler(\n",
    "        \"cosine\",\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,  # You can adjust the warmup steps if needed\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=roberta_dir/f\"LPFT_100_{lr}\",\n",
    "        # output_dir=roberta_dir/f\"LPFT_9_{lr}\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        logging_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        optimizers=(optimizer, scheduler),\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate(test_ds)\n",
    "    print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baselines",
   "language": "python",
   "name": "baselines"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
